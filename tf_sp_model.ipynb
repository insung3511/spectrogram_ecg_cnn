{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import stft\n",
    "from scipy import signal\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "\n",
    "EPOCH = 100\n",
    "KERNEL_SIZE = 3\n",
    "BATCH_SIZE = 128\n",
    "IMAGE_HEIGHT = 512\n",
    "IMAGE_WIDTH = 512\n",
    "\n",
    "DATA_PATH = \"./mit_data/\"\n",
    "SAVE_PATH = \"./spectrogram_plt/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parents_path = SAVE_PATH\n",
    "child_path = os.listdir(parents_path)\n",
    "\n",
    "temp_converted_img = list()\n",
    "temp_ann_list = list()\n",
    "converted_img = list()\n",
    "X = np.array(list())\n",
    "y = np.array(list())\n",
    "\n",
    "for pic_path in (child_path):\n",
    "    current_path = os.listdir(parents_path + pic_path)\n",
    "    print(\"[INFO] Current path : \" + parents_path + pic_path)\n",
    "    for file_name in tqdm(current_path):\n",
    "        path_for_array = parents_path + pic_path + \"/\" + file_name\n",
    "        \n",
    "        img_array = np.fromfile(path_for_array, np.uint8)\n",
    "        img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_resize = cv2.resize(img, dsize=(128, 128), interpolation=cv2.INTER_AREA)\n",
    "        temp_converted_img.append(img_resize / 255.0)\n",
    "        \n",
    "        check_ann = pic_path\n",
    "        \n",
    "        if check_ann == \"N\":            # Normal\n",
    "            temp_ann_list.append(0)\n",
    "        \n",
    "        elif check_ann == \"S\":          # Supra-ventricular\n",
    "            temp_ann_list.append(1)\n",
    "        \n",
    "        elif check_ann == \"V\":          # Ventricular\n",
    "            temp_ann_list.append(2)\n",
    "        \n",
    "        elif check_ann == \"F\":          # False alarm\n",
    "            temp_ann_list.append(3)\n",
    "        \n",
    "        else:                           # Unclassed \n",
    "            temp_ann_list.append(4)\n",
    "    \n",
    "    np.append(X, temp_converted_img)\n",
    "    np.append(y, temp_ann_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로에 폴더가 없으면 폴더 만들기\n",
    "def createDirectory(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, shuffle=True)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.33, random_state=42, shuffle=True )\n",
    "\n",
    "npx = np.array(X_train)\n",
    "npy = np.array(y_train)\n",
    "npx_vali = np.array(X_val)\n",
    "npy_vali = np.array(y_val)\n",
    "npx_test = np.array(X_test)\n",
    "npy_test = np.array(y_test)\n",
    "\n",
    "print(\"[SIZE]\\t\\tNpX lenght : {}\\n\\t\\tNpY length : {}\".format(npx.shape, npy.shape))\n",
    "print(\"[SIZE]\\t\\tX_validation length : {}\\n\\t\\ty_validation length : {}\".format(npx_vali.shape, npy_vali.shape))\n",
    "print(\"[SIZE]\\t\\tX_test length : {}\\n\\t\\ty_test length : {}\".format(npx_test.shape, npy_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train.unique)\n",
    "# print(X_train.unique)\n",
    "\n",
    "# class_names = y_train\n",
    "# print(class_names)\n",
    "# plt.figure(figsize=(10, 10))\n",
    "\n",
    "# for images, labels in X_train.take(1):\n",
    "#   for i in range(10):\n",
    "#     ax = plt.subplot(5, 5, i + 1)\n",
    "#     plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "#     plt.title(class_names[labels[i]])\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(30, 12))\n",
    "plt.suptitle(\"GOD DAMN\", fontsize=18)\n",
    "n = 0\n",
    "random.seed(11)\n",
    "\n",
    "for i in random.sample(range(6480), 16):\n",
    "    ax = plt.subplot(5, 5, n+1)\n",
    "    plt.plot(npx[i])\n",
    "    ax.set_title(str(npy[i]))\n",
    "    n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = (256, 256, 1)\n",
    "\n",
    "models = keras.Sequential([\n",
    "    # tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
    "    layers.Conv2D(8, kernel_size=(4, 4), activation='relu', input_shape=input_size),\n",
    "    layers.MaxPool2D(pool_size=(2, 2)),\n",
    "\n",
    "    layers.Conv2D(13, kernel_size=(2, 2), activation='relu', input_shape=input_size),\n",
    "    layers.MaxPool2D(pool_size=(2, 2)),\n",
    "\n",
    "    layers.Conv2D(13, kernel_size=(2, 2), activation='relu', input_shape=input_size),\n",
    "    layers.MaxPool2D(pool_size=(2, 2)),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(512, activation='relu'),\n",
    "    layers.Dense(5, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 콜백 설정\n",
    "outDir = './cheakpoint/lefms_model/' # 이 경로에 best 모델이 저장된다.\n",
    "model_names = outDir + 'weights-{val_accuracy:.4f}.h5'\n",
    "\n",
    "def get_callbacks(patience = 50):\n",
    "    earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=patience)\n",
    "    model_checkpoint = ModelCheckpoint(model_names, monitor='val_accuracy', verbose=1, save_best_only=True, period = 1)\n",
    "\n",
    "    callbacks = [model_checkpoint]\n",
    "    return callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c3cabcf2f29820bdd7faae982b59d335e0d215fb5382d93f3312fa3292e9b7f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
